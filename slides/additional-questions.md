# Доп вопросы прямиком от Риты

1. Можно называть любой из тех, которыми мы пользуемся. А в качестве аргумента указать признаки хороших инстументов
2. Я бы ответила "много, но исправленных", т.к. программа по сути представляет собой что-то типа минного поля, поэтому если ошибка выявлена и исправлена - это означает, что это самое поле больше расчищено и вероятность появления новых - меньше
3. примерно тоже самое, только можно делать слкедующий релиз, когда исправлены все баг фиксы
4. Примеры есть в 12 билете. Там есть отсылки какие СКВ к какому типеу относятся
5. clone, checkout, branch, add, commit, push, pull, remote, fetch
6. Предполагаю, потому что гит- распределенная СКВ, но это не точно
7. Главное, что гит - распределенная, а subvercion - централизованная... Можно еще сказать, что гит популярнее, что удобнее, быстрее
8. теоретически можно, но на практике, лишние накладки
9. Я бы сказала, что критические ошибки намного важнее текущих задач, поскольку решение некоторых из них может быть подорвано как раз, из-за этих ошибок. Поэтому в первую очередь надо заняться ими. Однако можно оставить несколько человек, которые будут продолжать заниматься текущими задачами, чтобы работа продолжалась. (ответ такой из-за анализа картинку с гитхаб флоу)
10. Фрейм ворки модульного тестирования, как и гугл тесты предназначены для создания автоматических тестов. Как пример, CUnit, Google Test (C++), JUnit (Java), PUnit(Python), NUnit (.Net)
11. Модульные тесты - проверка, что функция работает (например, что 2+2 = 4), интеграционный - что добавление/изменение какого-то модуля не сломает программу (например, что взаимодействие 2-х классов, хотя он может быть и примером модуля), системные - работа приложения в целом (например, запрос от 10000 пользователей одновременно не заруинит сервак)
12. СMake - для постройки проекта, Travis, BuildBot - сами сервера, Git - для проверки возможности интеграции в Travis, Google Test, CTest - чтобы посмотреть, как строятся тесты в различных системах, IDE - чтобы писать код, который нужен для автоматической сборки
13. Если я правильно понимаю - это относится к Waterfal, Night build, Pre-commit testing и тп. а это описывается в билете 3
14. Непрерывная - постоянная, частая. Имеентся в виду, что в идеале проект должен быть всегда актуальным. Интеграция - соединение, вставка. Т.е. Добавление изменений.
15. Статический анализ - как пример - проверка корректности, стиля кодирования... Динамический - покрытие кода тестами, утечки данных, производительность
16. надежность - человек может перепутать кнопки "Добавить", "отменить", или неправильно посчитать значение функции, а автоматика всегда сделает правильный выбор (если правильно закодирована). Дешевизна: Чтобы провести ручное тестирование, человеку надо заплатить, причем тестирование делается несколько раз. А можно нанять программистя написать тесты, которые будут проходить автоматически каждый раз и заплатить один раз. 
17. Скорость: тоже самое. Освобождение времени: Пока проходит автоматизация можно заняться чем-то другим. 
Управление файлами и проектами — ls, find, grep/ack, bash 
Текстовый редактор и средства редактирования — vim, awk, sort, column 
Компилятор и/или интерпретатор — gcc, perl 
Средства сборки — make 
Отладчик — gdb, valgrind, ltrace, lsof, pmap 
Контроль версий — diff, patch, svn, git
18. Если программа имеет много зависимостей, то динамическое (почему - 11 билет)
19. Как минимум потому, что исполняемый модуль аппаратно зависим и также засисит от версии операционной системы и ее компонент, поскольку программа, описынная в нем может вызывать библиотечные функции.
20. Вот тут минусы http://qaru.site/questions/30809/what-are-the-differences-between-autotools-cmake-and-scons
20. Маркдовн + гит могут изменяться вместе с основными файлами
22. Тут надо знать метасимволы для регулярных выражений и уметь строить различные шаблоны...



# Приведите примеры "хороших" инструментов, почему вы так считаете?

## Критерии хорошего инструмента

* Проверен временем
* Кросс-платформенный
* Допускает автоматизацию
* Быстрый
* Имеет открытый исходный код

## Примеры
* Текстовый редактор VSCode
    * Соответсвует критериям хорошего инструмента (кроме как проверен временем)
    * Имеет огромное число плагинов
    * Выделяется встроенным наличием Git и средств отладки на фоне других редакторов
* Pandoc
    * Позволяет конвертировать документы в огромное количество форматов
    * Используется в командной строке
    * Легковесен и достаточно мощен
* Офисный пакет LibreOffice
    * Абсолютно бесплатен
    * При этом не уступает по возможностям Microsoft Office
    * (Субъективно) Работает шустрее чем офисный пакет Microsoft Office



# В стабильности какого продукта вы уверены больше, для которого зарегистрировано много или мало ошибок? Почему?

Разработчики каждого продукта стараются свести к минимуму число ошибок. Но малое количество ошибок не показатель качества и
востребанности продукта. 

Пользователи, действительно заинтересованные в продукте будут сообщать о найденных багах и этот
процесс бесконечен. Но этот процесс ведет к постепенному увеличения качества и стабильности продукта.

Невостребованный продукт постепенно избавляется от старых багов, но не получает новые от пользователей, соответсвенно этот
продукт меньше протестирован пользователями, чем востребованный продукт.



# Примеры СКВ кроме SVN и Git

## TFS - Team Foundation Server

Отображение рабочей области:

* Простота настройки и понимания
* Легко протестировать изменение библиотеки в нескольких продуктах.
* Легко получать последние изменения в библиотеке и отправлять изменения в библиотеку
* Никакой трассируемости или, по меньшей мере, меньшей проходимости, например. если в продукте A было внесено изменение в библиотеку, как отслеживать это изменение в продукте B
* Изменения в библиотеках могут влиять на продукты неконтролируемым образом.
* Сборка становится сложнее
* Каждый пользователь должен настроить свое рабочее пространство индивидуально (но в TFS 2012 Power Tools есть шаблоны рабочей области).

Отображение папок:

* Все, что необходимо, настраивается в данной ветке
* Изоляция между продуктами и отраслями.
* Сборка упрощена.
* Управление изменениями
* Требуется больше дискового пространства
* Требуется больше администрирования в виде ветвления/слияния и настройки веток.

## Mercurial (Hg)

Mercurial схожа с Git своей распределённой структурой, однако имеет более __строгий подход к ветвлению__ нежели в Git.

Это одновременно и плюс и минус
* Плюс — сложнее создать запутанную структуру в репозитории
* Минус — нет возможности перемещать правки и делать некоторые другие вещи за которые многие так любят Git

Mercurial немного хуже поддерживается разработчиками IDE, а так же имеет более скромный набор графических оболочек в отличие от Git и SVN.

## Perforce (P4)

### Плюсы

* Тотальный контроль над всем;
* Идущая от разработчиков графическая оболочка;
* Возможность удобно комбинировать работу над несколькими модулями;
* Удобная работа со списками изменений.

### Минусы

* Трудно заставить себя привыкнуть к тому, что инструмент диктует тебе как работать, а не наоборот;
* Когда теряется связь с сервером, прекращается продуктивная работа



# Укажите последовательность git команд, которые требовалось вызвать при работе над 2й и 3й лабораторными

* git checkout master
    * Переход на ветку master форкнутого репозитория
* git add remote upstream https://github.com/UNN-VMK-Software/devtools-course-practice.git
    * Добавление ссылки на главный репозиторий для получения актуальных данных
* git fetch upstream
    * Получение и сохранение на локальной машине последних изменений из upstream, т.е. с ветки master главного репозитория
* git merge upstream/master
    * Вливание полученных изменений в текущую ветку (master)
* git commit -m "some info"
    * Добавление коммита
* git push origin master
    * Фиксирование локальных изменений на удаленном репозитории
* git checkout -b lab2-YOUR-NAME
    * Создание и переход на ветку lab2-YOUR-NAME, имеющую актуальные файлы



# Почему слияние работает лучше в Git, чем в Subversion

It is required to manually specify the range of revisions when you merge two branches in Subversion 
False. An outdated myth.
Starting with Subversion 1.5 (released in June 2008), Subversion implements the merge tracking feature and manual revision range specification is not required anymore. Moreover, Subversion 1.8 (released in June 2013) provides automatic reintegration merges that further simplify merging changes between branches.

Merge operation is always painful in Subversion 
Spotted problems exist.
In most cases merges become painful in Subversion only if you have file or folder renames in the merged branches. Due to historical reasons, Subversion doesn’t properly track file and folder renames (mostly because file renames rarely happened before refactorings were invented). Best practices to prevent tree conflicts during merge are simple: limit file and folder renames in branches, prefer to refactor code in the trunk. It is important to note that improved merging and better tree conflict handling are the hot features for the next Subversion release.

SVN отслеживает файлы, а Git отслеживает изменения содержимого . Он достаточно умен, чтобы отслеживать блок кода, который был реорганизован из одного класса / файла в другой. Они используют два совершенно разных подхода к отслеживанию вашего источника.


# В чем разница между ветками в Git и Subversion

Branches are expensive in Subversion
False. A myth.

Branches in Subversion are implemented with Copy-On-Write strategy (referred to as ‘Cheap Copies’ in the svnbook). No matter how large a repository or project is, it takes a constant amount of time and space to make a branch. In fact, Subversion branches are extremely cheap beginning with version 1.0 and you can branch even for small bugfixes in a very busy and large project.

Вывод – создание бранчи происходит быстрей чем вы глазом моргнуть успеете. Мне показалось, что если вся операция занимает меньше 0,01 секунды то тут и сравнивать нечего. Но почему-то на заявление о дороговизне бранчей в svn, сайт проверил только их создание. Но есть другие операции, например клонирование( или svn checkout). В этом эксперименте все происходит локально, возможное влияние сети исключено.


# Можно ли использовать GitHub Flow при централизованном рабочем процессе (Centralized Workflow?)

В теории можно, но это крайне неудобно
Во первых потому что главный и единственный репозиторий будет содержать огромное число веток (большинство feature-branch)
Во вторых процесс немедленного развертывания может быть осложнен
В третьих опасно хранить всего один репозиторий. Он должен иметь хотя бы несколько форков



# Если задачи на ближайший релиз уже отобраны, и работа идет, как быть если находится несколько критических ошибок? Времени точно не хватит на запланированные задачи и критические ошибки.

Критические ошибки должны быть исправлены в первую очередь

Стабильность и полноценная работоспособность программы всегда должны быть приоритетнее чем новые фичи и дизайн



# Какой смысл несут слова "непрерывная" и "интеграция" в названии практики CI?

Каждое изменение __должно интегрироваться__ 
* Слово continuous в термине Continuous Integration означает «непрерывный/непрекращающийся»
    * Это означает, что в идеале сборка вашего проекта должна идти буквально все время
    * Каждое изменение в системе контроля версий должно интегрироваться без пропусков или задержек
    * Организация ночных сборок — это хорошая практика, но это не continuous integration
        * Результаты такой ночной сборки будут доступны лишь на следующий день, когда их актуальность для разработчиков уже значительно снижена
    * На практике довольно часто реализуют оба процесса и непрерывную интеграция и ночные сборки — более редкую интеграцию
    * Принцип непрерывной интеграции не выполним без другого условия — «Сборка должна идти быстро».



# Чем отличаются статические и динамические анализаторы кода. Приведите примеры ошибок, которые можно найти только одним видом инструментов, но не другим.

__ВОПРОС ГОВНО__
* Динамическое связывание может сократить общее потребление ресурсов
    * Несколько процессов должны иметь одну и ту же библиотеку одинаковой версии
    * Ресурсы включают дисковое пространство, ОЗУ и пространство кеша
    * Динамический компоновщик должен быть достаточно гибким
* исправления и обновления ошибок в библиотеках распространяются, чтобы улучшить ваш продукт, не требуя, чтобы вы отправляли что-либо

* статическое связывание означает, что вы можете знать, что код будет работать в очень ограниченных средах (в начале процесса загрузки или в режиме восстановления)
